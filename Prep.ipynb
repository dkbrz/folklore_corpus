{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "con = sqlite3.connect('FDB.db')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pymystem3' from '/home/dkbrz/.local/lib/python3.6/site-packages/pymystem3/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'pos', 'ADV': 'pos', 'ADVPRO': 'pos', 'ANUM': 'pos', 'APRO': 'pos', 'COM': 'pos', 'CONJ': 'pos', 'INTJ': 'pos', 'NUM': 'pos', 'PART': 'pos', 'PR': 'pos', 'S': 'pos', 'SPRO': 'pos', 'V': 'pos', 'praes': 'tense', 'inpraes': 'tense', 'praet': 'tense', 'nom': 'case', 'gen': 'case', 'dat': 'case', 'acc': 'case', 'ins': 'case', 'abl': 'case', 'part': 'case', 'loc': 'case', 'voc': 'case', 'sg': 'num', 'pl': 'num', 'ger': 'vform', 'inf': 'vform', 'partcp': 'vform', 'indic': 'vform', 'imper': 'vform', 'brev': 'aform', 'plen': 'aform', 'poss': 'aform', 'supr': 'degree', 'comp': 'degree', '1p': 'pers', '2p': 'pers', '3p': 'pers', 'm': 'gender', 'f': 'gender', 'n': 'gender', 'ipf': 'asp', 'pf': 'asp', 'act': 'voice', 'pass': 'voice', 'tran': 'trans', 'intr': 'trans', 'parenth': 'type', 'praed': 'type', 'abbr': 'type', 'mf': 'type', 'awkw': 'form', 'dist': 'form', 'geo': 'sem', 'persn': 'sem', 'patrn': 'sem', 'famn': 'sem', 'obsc': 'style', 'inform': 'style', 'rare': 'style', 'obsol': 'style'}\n"
     ]
    }
   ],
   "source": [
    "with open('./raw_data/tags.csv','r') as f:\n",
    "    tags = f.readlines()\n",
    "    result = {t.split()[0]:t.split()[1] for t in tags}\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import informants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_1 = sqlite3.connect('./raw_data/ABM.sqlite')\n",
    "db_1_cur = db_1.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_1_cur.execute('SELECT * FROM tblInformators')\n",
    "inf = db_1_cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f09744a6960>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.executemany('INSERT INTO Informators (code, name, city, bio, old_id) VALUES (?,?,?,?,?)', inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import texts to database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_1_cur.execute('SELECT * FROM tblCards')\n",
    "texts = db_1_cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [Шёл разговор о пьющей женщине из Карпиловки, Г назвал её «Ма\\ндыка».] [Ма\\ндыка?] [Г:] Да. [Что это значит?] [Г, смеясь:] Ну, манды\\ка — э\\то… [А одновременно:] Коро\\че её ёб… [Г:] Все. [А:] Всё село\\. [Г:] Да, коро\\че… [А:] И… и… и из на\\шего, и коро\\че там… [Г:] Все подря\\д. [А:] Да. [А и Г одновременно:] Кому\\ не лень. [А:] Буты\\лку на стол, и… [Она пьёт, да?] [Г:] Ну, она\\ по-чё\\рному пьёт.\n",
      "1 \n",
      "2 \n",
      "3 \n",
      "4 XXIa\n",
      "5 5 б\n",
      "6 Брянская обл., Злынковский р-н, Спиридонова Буда\n",
      "7 2017\n",
      "8 А\n",
      "9 Г\n",
      "10 \n",
      "11 \n",
      "12 МЭГ\n",
      "13 НВП\n",
      "14 СЕ\n",
      "15 ФЕЕ\n",
      "16 1\n",
      "17 \n",
      "18 \n",
      "19 \n",
      "20 \n"
     ]
    }
   ],
   "source": [
    "for key, value in enumerate(texts[0]):\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in texts:\n",
    "    new_row = (\n",
    "        'АБМ',\n",
    "        row[16],\n",
    "        row[6],\n",
    "        row[0],\n",
    "        row[7],\n",
    "        restore_label(row[5]),\n",
    "        row[4],\n",
    "        ', '.join(i for i in row[8:12] if i != ''),\n",
    "        ', '.join(i for i in row[12:16] if i != '') \n",
    "    )\n",
    "    texts2.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f09744a6960>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.executemany('INSERT INTO Texts (leader, old_id, city, raw_text, year, question, list, informants, collectors)\\\n",
    "VALUES (?,?,?,?,?,?,?,?,?)', texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. change question labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_label(label):\n",
    "    label = label.split(',')\n",
    "    result = []\n",
    "    #print(label)\n",
    "    for i in label:\n",
    "        q = re.findall('^([0-9]*)[^0-9a-zа-я]*?([ a-zа-я\\.]*?)$', i.strip())[0]\n",
    "        #print (q)\n",
    "        result.append((q[0].strip(), q[1].strip()))\n",
    "    for key, value in enumerate(result):\n",
    "        if value[0] == '':\n",
    "            result[key] = (result[key-1][0].strip(), value[1].strip())\n",
    "    #print (result)\n",
    "    return ', '.join(''.join(i) for i in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15а, 19а, 19в, 30а, 14а, 25а, 25б, 16а'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_label('15 а, 19 а, в, 30 а, 14 а, 25 а, б, 16 а')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10', 'б')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('^([0-9]*)[^0-9a-zа-я]*?([a-zа-я]*?)$',label[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Вопрос'] = restore_label(data['Вопрос'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'у% Я́ цветоу%ок не брал и цветоу%'\n",
    "len ([i['text'] for i in m.analyze(a)]) == len(m.analyze(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_v(text):\n",
    "    k = [i['text'] for i in m.analyze(text)]\n",
    "    r = ['empty']\n",
    "    for i in k:\n",
    "        if '%' not in i:\n",
    "            if r[-1][-1] != '%':\n",
    "                r += [i]\n",
    "            else:\n",
    "                r[-1]+=i\n",
    "        else:\n",
    "            r[-1]+='%'\n",
    "            if i != '%':\n",
    "                r += [i.replace('%','')] \n",
    "    return r[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['у%',\n",
       " ' ',\n",
       " 'Я́',\n",
       " ' ',\n",
       " 'цветоу%ок',\n",
       " ' ',\n",
       " 'не',\n",
       " ' ',\n",
       " 'брал',\n",
       " ' ',\n",
       " 'и',\n",
       " ' ',\n",
       " 'цветоу%',\n",
       " '\\n']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_v(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в Я́ цветовок не брал и цветов\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(norm_v(a)).replace('у%','в')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accents = {'А́ ':'А', 'Е́ ':'Е','И́ ':'И','О́ ':'О','У́ ':'У','Ы́ ':'Ы','Э́ ':'Э','Ю́ ':'Ю','Я́':'Я',\n",
    "           'а́':'а','е́':'е','и́':'и','о́':'о', 'у́':'у', 'ы́':'ы', 'э́':'э', 'ю́':'ю', 'я́':'я'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accents = {item[1]+'\\\\': item[0] for item in accents.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '[Вы вчера рассказывали историю с цветами, что это было?] [А:] Ну, э\\то как бы э… не зна\\ю, и\\ли приме\\та и\\ли… ну, что-то… вот когда\\ моего\\ дя\\дю хорони\\ли э… то\\же, ну, кода\\ хорони\\ли его\\, хоте\\ли то\\же цветы\\ положи\\ть и\\ли что-то хоте\\ли положи\\ть туда\\, вро\\де, цветы\\, она\\ стро\\го запрети\\ла, ну там, же\\нщина, кото\\рая всё по э\\тим, по похорона\\м от, она\\ сказа\\ла: «Стро\\го нельзя\\». Ну, ти\\па, ско\\ка положи\\ли тогда\\, ну, в про\\шлый раз там что-то, в моги\\лу цвето\\у%, сто… че\\рез сто\\ка дней ро\\дственник у\\мер и\\ли чт… ну, коро\\че, что-то тако\\е, и\\ли когда\\ цветы\\ завя\\ли, ро\\дственник у\\мер, что-то… [Соб.: Ты что-то рассказывал, что она букет кинула в гроб…] Ну да, э\\то вон, Гри\\ша, да. [Соб.: А, Гриша.] [Г:] Я расска\\зывал от что, ну, они\\ сказа\\ли то, что кода\\ она\\ ски\\нула, ско\\ка, то\\ ли де\\вять цвето\\в, ну, в моги\\лу получа\\ется кода\\... все песо\\к кида\\ли, а она\\ цветы\\ ски\\нула. И по\\сле э\\того у нас, ну, начало\\сь, вот э\\ти де\\вять пацано\\в у\\мерло. И\\менно вот из-за э\\того. Говоря\\т то, что и\\менно из-за э\\того, то что…'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'И\\\\', 'а\\\\', 'е\\\\', 'и\\\\', 'о\\\\', 'у\\\\', 'ы\\\\', 'э\\\\', 'я\\\\'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = set(re.findall('.\\\\\\\\', a))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s:\n",
    "    a = a.replace(i, get_accents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Вы вчера рассказывали историю с цветами, что это было?] [А:] Ну, э́то как бы э… не зна́ю, и́ли приме́та и́ли… ну, что-то… вот когда́ моего́ дя́дю хорони́ли э… то́же, ну, кода́ хорони́ли его́, хоте́ли то́же цветы́ положи́ть и́ли что-то хоте́ли положи́ть туда́, вро́де, цветы́, она́ стро́го запрети́ла, ну там, же́нщина, кото́рая всё по э́тим, по похорона́м от, она́ сказа́ла: «Стро́го нельзя́». Ну, ти́па, ско́ка положи́ли тогда́, ну, в про́шлый раз там что-то, в моги́лу цвето́у%, сто… че́рез сто́ка дней ро́дственник у́мер и́ли чт… ну, коро́че, что-то тако́е, и́ли когда́ цветы́ завя́ли, ро́дственник у́мер, что-то… [Соб.: Ты что-то рассказывал, что она букет кинула в гроб…] Ну да, э́то вон, Гри́ша, да. [Соб.: А, Гриша.] [Г:] Я расска́зывал от что, ну, они́ сказа́ли то, что кода́ она́ ски́нула, ско́ка, то́ ли де́вять цвето́в, ну, в моги́лу получа́ется кода́... все песо́к кида́ли, а она́ цветы́ ски́нула. И по́сле э́того у нас, ну, начало́сь, вот э́ти де́вять пацано́в у́мерло. И́ менно вот из-за э́того. Говоря́т то, что и́менно из-за э́того, то что…'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = re.sub('\\[','\\n[',a)\n",
    "\n",
    "a = re.sub('[^:]\\]',']\\n',a)\n",
    "\n",
    "a = re.sub('[ ]*?\\n','\\n',a)\n",
    "\n",
    "a = re.sub('\\n[ \\n]*','\\n',a).strip()\n",
    "\n",
    "a = a.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[А:]\n",
      "[Г:]\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    r = re.findall('\\[.*?\\]', i)\n",
    "    if len(r) > 0:\n",
    "        if re.match('^\\[[A-ZА-Я-0-9]{1,5}:\\]$', r[0]):\n",
    "            print (r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'ну', 'wt': 0.9991076439, 'gr': 'PART='}],\n",
       "  'text': 'Ну'},\n",
       " {'text': ', '},\n",
       " {'analysis': [{'lex': 'это',\n",
       "    'wt': 0.7809833731,\n",
       "    'gr': 'SPRO,sg,n,inan=(acc|nom)'}],\n",
       "  'text': 'э́то'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.analyze(a)[22:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneSentence:\n",
    "    def __init__(self, text, prefix):\n",
    "        self.show_prefix = False\n",
    "        self.prefix = prefix\n",
    "        self.text = text\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '({}\\t{}\\t{})'.format(str(self.show_prefix), self.prefix, self.text)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "        \n",
    "class Sentences:\n",
    "    def __init__(self,text):\n",
    "        self.sentences = self._assign_turns(text)\n",
    "        #print (self.turns)\n",
    "        \n",
    "    def _split_turns(self, text):\n",
    "        global get_accents\n",
    "        tags = set(re.findall('.\\\\\\\\', text))\n",
    "        for i in tags:\n",
    "            text = text.replace(i, get_accents[i])\n",
    "        text = re.sub('\\[','\\n[',text)\n",
    "        text = re.sub('[^:]\\]',']\\n',text)\n",
    "        text = re.sub('[ ]*?\\n','\\n',text)\n",
    "        text = re.sub('\\n[ \\n]*','\\n',text).strip()\n",
    "        text = text.split('\\n')\n",
    "        return text\n",
    "    \n",
    "    def _assign_turns(self,text):\n",
    "        text = self._split_turns(text)\n",
    "        prefixes = []\n",
    "        result = []\n",
    "        for i in text:\n",
    "            r = re.findall('\\[.*?\\]', i)\n",
    "            if len(r) > 0:\n",
    "                if re.match('^\\[[A-ZА-Я-0-9]{1,5}:\\]$', r[0]):\n",
    "                    prefixes.append(r[0])\n",
    "                    sentences = sent_tokenize(i.replace(r[0], '').strip())\n",
    "                    result.append(OneSentence(sentences[0], prefixes[-1]))\n",
    "                    result[-1].show_prefix = True\n",
    "                    for x in sentences[1:]:\n",
    "                        result.append(OneSentence(x, prefixes[-1]))\n",
    "                else:\n",
    "                    result.append(OneSentence(r[0], ''))\n",
    "            else:\n",
    "                sentences = sent_tokenize(i.strip())\n",
    "                result.append(OneSentence(sentences[0], prefixes[-1]))\n",
    "                result[-1].show_prefix = True\n",
    "                for x in sentences[1:]:\n",
    "                    result.append(OneSentence(x, prefixes[-1]))\n",
    "        return result\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '\\n\\n'.join([str(i) for i in self.sentences])\n",
    "    \n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedSentences:\n",
    "    \n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        \n",
    "    def _norm_v(self, text):\n",
    "        global accents\n",
    "        k = [i['text'] for i in m.analyze(text)]\n",
    "        r = ['empty']\n",
    "        for i in k:\n",
    "            if '%' not in i:\n",
    "                if r[-1][-1] != '%':\n",
    "                    r += [i]\n",
    "                else:\n",
    "                    r[-1]+=i\n",
    "            else:\n",
    "                r[-1]+='%'\n",
    "                if i != '%':\n",
    "                    r += [i.replace('%','')] \n",
    "        norm = ''.join(r[1:]).replace('у%','в').replace('У%','В')\n",
    "        for i in accents:\n",
    "            norm = norm.replace(i, accents[i])\n",
    "        return r[1:], m.analyze(norm)\n",
    "    \n",
    "    def _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_v(text):\n",
    "        global accents\n",
    "        k = [i['text'] for i in m.analyze(text)]\n",
    "        r = ['empty']\n",
    "        for i in k:\n",
    "            if '%' not in i:\n",
    "                if r[-1][-1] != '%':\n",
    "                    r += [i]\n",
    "                else:\n",
    "                    r[-1]+=i\n",
    "            else:\n",
    "                r[-1]+='%'\n",
    "                if i != '%':\n",
    "                    r += [i.replace('%','')] \n",
    "        norm = ''.join(r[1:]).replace('у%','в').replace('У%','В')\n",
    "        for i in accents:\n",
    "            norm = norm.replace(i, accents[i])\n",
    "        return r[1:], m.analyze(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['У%',\n",
       "  ' ',\n",
       "  'Я́',\n",
       "  ' ',\n",
       "  'цветоу%ок',\n",
       "  ' ',\n",
       "  'не',\n",
       "  ' ',\n",
       "  'брал',\n",
       "  ' ',\n",
       "  'и',\n",
       "  ' ',\n",
       "  'цвето́у%',\n",
       "  '\\n'],\n",
       " [{'analysis': [{'lex': 'в',\n",
       "     'wt': 8.212235587e-06,\n",
       "     'gr': 'S,abbr=(abl,pl|abl,sg|acc,pl|acc,sg|dat,pl|dat,sg|gen,pl|gen,sg|ins,pl|ins,sg|nom,pl|nom,sg)'}],\n",
       "   'text': 'В'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'lex': 'я',\n",
       "     'wt': 2.837188476e-05,\n",
       "     'gr': 'S,abbr=(abl,pl|abl,sg|acc,pl|acc,sg|dat,pl|dat,sg|gen,pl|gen,sg|ins,pl|ins,sg|nom,pl|nom,sg)'}],\n",
       "   'text': 'Я'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'lex': 'цветовка',\n",
       "     'wt': 1,\n",
       "     'qual': 'bastard',\n",
       "     'gr': 'S,geo,f,inan=gen,pl'}],\n",
       "   'text': 'цветовок'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'lex': 'брать',\n",
       "     'wt': 1,\n",
       "     'gr': 'V,ipf,tran=praet,sg,indic,m'}],\n",
       "   'text': 'брал'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'},\n",
       "  {'text': ' '},\n",
       "  {'analysis': [{'lex': 'цветок', 'wt': 0.777675558, 'gr': 'S,m,inan=gen,pl'}],\n",
       "   'text': 'цветов'},\n",
       "  {'text': '\\n'}])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'У% Я́ цветоу%ок не брал и цвето́у%'\n",
    "norm_v(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False\t\t[Вы вчера рассказывали историю с цветами, что это было])\n",
       "\n",
       "(True\t[А:]\tНу, э́то как бы э… не зна́ю, и́ли приме́та и́ли… ну, что-то… вот когда́ моего́ дя́дю хорони́ли э… то́же, ну, кода́ хорони́ли его́, хоте́ли то́же цветы́ положи́ть и́ли что-то хоте́ли положи́ть туда́, вро́де, цветы́, она́ стро́го запрети́ла, ну там, же́нщина, кото́рая всё по э́тим, по похорона́м от, она́ сказа́ла: «Стро́го нельзя́».)\n",
       "\n",
       "(False\t[А:]\tНу, ти́па, ско́ка положи́ли тогда́, ну, в про́шлый раз там что-то, в моги́лу цвето́у%, сто… че́рез сто́ка дней ро́дственник у́мер и́ли чт… ну, коро́че, что-то тако́е, и́ли когда́ цветы́ завя́ли, ро́дственник у́мер, что-то…)\n",
       "\n",
       "(False\t\t[Соб.: Ты что-то рассказывал, что она букет кинула в гроб])\n",
       "\n",
       "(True\t[А:]\tНу да, э́то вон, Гри́ша, да.)\n",
       "\n",
       "(False\t\t[Соб.: А, Гриша])\n",
       "\n",
       "(True\t[Г:]\tЯ расска́зывал от что, ну, они́ сказа́ли то, что кода́ она́ ски́нула, ско́ка, то́ ли де́вять цвето́в, ну, в моги́лу получа́ется кода́... все песо́к кида́ли, а она́ цветы́ ски́нула.)\n",
       "\n",
       "(False\t[Г:]\tИ по́сле э́того у нас, ну, начало́сь, вот э́ти де́вять пацано́в у́мерло.)\n",
       "\n",
       "(False\t[Г:]\tИ́ менно вот из-за э́того.)\n",
       "\n",
       "(False\t[Г:]\tГоворя́т то, что и́менно из-за э́того, то что…)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Sentences(a)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = A.sentences[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_stress(text):\n",
    "    text = text.replace('\\\\', '')\n",
    "    return text\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.replace('[', '\\n[')\n",
    "    text = re.sub('\\n{2,}','\\n')\n",
    "    text = text.split('\\n')\n",
    "    text = [sent_tokenize(chunk) for chunk in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(code):\n",
    "    cur.execute('SELECT id, gender, birthyear FROM Informants WHERE code=\\\"{}\\\"'.format(code))\n",
    "    indiv = cur.fetchone()\n",
    "    return {'id':indiv[0], 'gender':indiv[1], 'year':indiv[2]}\n",
    "\n",
    "def get_col_id(code):\n",
    "    cur.execute('SELECT id FROM Collectors WHERE code=\\\"{}\\\"'.format(code))\n",
    "    indiv = cur.fetchone()\n",
    "    return indiv[0]\n",
    "\n",
    "def joint_meta(informants, collectors):\n",
    "    informants = informants.split(',')\n",
    "    result = {'inf':{}, 'col':{}}\n",
    "    for i in informants:\n",
    "        result['inf'][i.strip()] = get_meta(code)\n",
    "    for i in collectors:\n",
    "        result['col'][i.strip()] = get_col_id(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_text(text, sentence):\n",
    "    text = text\n",
    "    for word in words:\n",
    "            if 'wf' not in word:\n",
    "                continue\n",
    "            word['off_start'] = len(text)\n",
    "            if word['wtype'] == 'word':\n",
    "                text += word['wf'] + ' '\n",
    "                word['off_end'] = len(text) - 1\n",
    "            else:\n",
    "                if word['wf'].startswith(('(', '[', '{', '<', '“')):\n",
    "                    text += word['wf']\n",
    "                    word['off_end'] = len(text)\n",
    "                elif word['wf'].startswith((')', ']', '}', '>', '.', ',', '?', '!', '”', '…')):\n",
    "                    if text.endswith(' '):\n",
    "                        word['off_start'] -= 1\n",
    "                        text = text[:-1]\n",
    "                    text += word['wf'] + ' '\n",
    "                    word['off_end'] = len(text) - 1\n",
    "                else:\n",
    "                    text += word['wf'] + ' '\n",
    "                    word['off_end'] = len(text) - 1\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana(analysis):\n",
    "    global tags\n",
    "    result = {}\n",
    "    for i in analysis:\n",
    "        if i in tags:\n",
    "            result[tags[i]] = i\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(parsed):\n",
    "    result = []\n",
    "    for i in parsed:\n",
    "        if 'analysis' in i:\n",
    "            r = {}\n",
    "            r['wf'] = i['text']\n",
    "            r['wtype'] = i['word']\n",
    "            r['ana'] = []\n",
    "            for var in i['analysis']:\n",
    "                r['ana'].append(ana(var))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence, meta):\n",
    "    result = {'meta':{}, 'words':[]}\n",
    "    if not re.match('^\\[.*?\\]$', sentence):\n",
    "        sentence = sentence.split(']')\n",
    "        sentence[0] = sentence[0]+']'\n",
    "        parsed = tokens(m.analyze(sentence[1]))\n",
    "    else:\n",
    "        parsed = tokens(m.analyze(sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
